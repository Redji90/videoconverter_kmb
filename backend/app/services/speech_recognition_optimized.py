"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- Faster-Whisper –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
- Speaker Diarization (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–æ–ª—è–º)
"""
import os
from typing import Optional, Dict, List
from pathlib import Path

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ XET –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –∑–∞–≤–∏—Å–∞–Ω–∏–µ–º –∑–∞–≥—Ä—É–∑–æ–∫ –Ω–∞ Windows
# XET —á–∞—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç —Ç–∞–π–º–∞—É—Ç—ã –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

# –í–ê–ñ–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º WHISPER_CACHE_DIR –î–û –∏–º–ø–æ—Ä—Ç–∞ whisper
# Whisper –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–æ—Ç –ø—É—Ç—å –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
# –ï—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ config
if "WHISPER_CACHE_DIR" not in os.environ:
    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ config
    try:
        import sys
        # –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å config
        config_path = Path(__file__).parent.parent.parent / "config.py"
        if config_path.exists():
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
            sys.path.insert(0, str(config_path.parent))
            try:
                from config import WHISPER_CACHE_DIR as CONFIG_CACHE_DIR
                if CONFIG_CACHE_DIR:
                    os.environ["WHISPER_CACHE_DIR"] = CONFIG_CACHE_DIR
            except ImportError:
                # –ï—Å–ª–∏ –∏–º–ø–æ—Ä—Ç –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é
                import re
                with open(config_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # –ò—â–µ–º WHISPER_CACHE_DIR = "E:\\whisper-models"
                    match = re.search(r'WHISPER_CACHE_DIR.*?=.*?["\']([^"\']+)["\']', content)
                    if match:
                        os.environ["WHISPER_CACHE_DIR"] = match.group(1)
    except Exception:
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å
        default_path = "E:\\whisper-models"
        if Path(default_path).exists():
            os.environ["WHISPER_CACHE_DIR"] = default_path

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ faster-whisper (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    # –í–ê–ñ–ù–û: –ò–º–ø–æ—Ä—Ç whisper –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∑–¥–µ—Å—å, –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ WHISPER_CACHE_DIR
    import whisper

# –í–ê–ñ–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á torch.load –î–û –∏–º–ø–æ—Ä—Ç–∞ WhisperX/pyannote
# —á—Ç–æ–±—ã –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –≤—ã–∑–æ–≤–∞–º torch.load –≤ —ç—Ç–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –º–æ–¥—É–ª—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
# –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫—É—Ä—Å–∏—é –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö
_torch_load_original_global = None

try:
    import torch
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –î–û –ø–∞—Ç—á–∞ (–≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥—É–ª—è)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–Ω–∞ –µ—â–µ –Ω–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞)
    if _torch_load_original_global is None:
        _torch_load_original_global = torch.load
    
    # –ü–∞—Ç—á –¥–ª—è torch.load —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å weights_only=False –¥–ª—è pyannote
    # PyTorch 2.6+ —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è weights_only=False –¥–ª—è –º–æ–¥–µ–ª–µ–π pyannote
    def _torch_load_patched_final(*args, **kwargs):
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º weights_only=False –¥–ª—è pyannote (–¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ HuggingFace)
        kwargs['weights_only'] = False
        # –ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º torch.load, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ä–µ–∫—É—Ä—Å–∏–∏!
        return _torch_load_original_global(*args, **kwargs)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è PyTorch 2.6+ (–≤–µ—Ä—Å–∏—è 2.8.0 —Ç–æ—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ–≥–æ)
    try:
        torch_version = torch.__version__
        major, minor = map(int, torch_version.split('.')[:2])
        if major > 2 or (major == 2 and minor >= 6):
            torch.load = _torch_load_patched_final
    except:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–µ—Ä—Å–∏—é, –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        # (–ª—É—á—à–µ –ø–µ—Ä–µ—Å—Ç—Ä–∞—Ö–æ–≤–∞—Ç—å—Å—è)
        torch.load = _torch_load_patched_final
except Exception:
    # –ï—Å–ª–∏ –ø–∞—Ç—á –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ (–º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –æ—à–∏–±–∫—É –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π)
    pass

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ WhisperX –¥–ª—è diarization
try:
    import whisperx
    WHISPERX_AVAILABLE = True
except ImportError:
    WHISPERX_AVAILABLE = False

# –ü—Ä–æ—Å—Ç–∞—è diarization –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—É–∑ (fallback, –µ—Å–ª–∏ WhisperX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
try:
    from .simple_diarization import simple_diarization, group_by_speakers
    SIMPLE_DIARIZATION_AVAILABLE = True
except ImportError:
    SIMPLE_DIARIZATION_AVAILABLE = False


class OptimizedSpeechRecognitionService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏"""
    
    def __init__(self, cache_dir: Optional[str] = None, use_gpu: bool = False, device: str = "auto"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            cache_dir: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            use_gpu: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ ("cuda", "cpu", "auto")
        """
        self.models = {}
        self.default_model = "base"
        self.cache_dir = Path(cache_dir) if cache_dir else None
        self.use_gpu = use_gpu
        self.device = device
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if device == "auto":
            if use_gpu and FASTER_WHISPER_AVAILABLE:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
                try:
                    import torch
                    self.device = "cuda" if torch.cuda.is_available() else "cpu"
                except:
                    self.device = "cpu"
            else:
                self.device = "cpu"
        
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            os.environ["WHISPER_CACHE_DIR"] = str(self.cache_dir)
            print(f"–ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤: {self.cache_dir}")
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π –≤ –∫—ç—à–µ (–¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ Whisper)
            if not FASTER_WHISPER_AVAILABLE:
                for model_name in ["tiny", "base", "small", "medium", "large"]:
                    model_file = self.cache_dir / f"{model_name}.pt"
                    if model_file.exists():
                        size_gb = model_file.stat().st_size / (1024 * 1024 * 1024)
                        print(f"  ‚úì –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å {model_name}.pt ({size_gb:.2f} GB)")
        
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {'Faster-Whisper' if FASTER_WHISPER_AVAILABLE else 'Standard Whisper'}")
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        if WHISPERX_AVAILABLE:
            print(f"Speaker Diarization: –î–æ—Å—Ç—É–ø–µ–Ω (WhisperX)")
        elif SIMPLE_DIARIZATION_AVAILABLE:
            print(f"Speaker Diarization: –î–æ—Å—Ç—É–ø–µ–Ω (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—É–∑)")
        else:
            print(f"Speaker Diarization: –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    def load_model(self, model_name: str = "base"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        if model_name not in self.models:
            print(f"[LOAD_MODEL] –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {model_name}")
            print(f"[LOAD_MODEL] FASTER_WHISPER_AVAILABLE: {FASTER_WHISPER_AVAILABLE}")
            print(f"[LOAD_MODEL] –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            if FASTER_WHISPER_AVAILABLE:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º faster-whisper (–±—ã—Å—Ç—Ä–µ–µ)
                download_path = str(self.cache_dir) if self.cache_dir else None
                
                # –û—Ç–∫–ª—é—á–∞–µ–º XET –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –∑–∞–≤–∏—Å–∞–Ω–∏–µ–º
                # –≠—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º WhisperModel
                os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Faster-Whisper
                if download_path:
                    model_path = Path(download_path) / model_name
                    if model_path.exists() and any(model_path.iterdir()):
                        print(f"  –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å Faster-Whisper –≤: {model_path}")
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ .pt —Ñ–∞–π–ª (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper)
                        pt_file = Path(download_path) / f"{model_name}.pt"
                        if pt_file.exists():
                            print(f"  ‚ö† –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª {model_name}.pt (—Ñ–æ—Ä–º–∞—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ Whisper)")
                            print(f"  ‚ö† Faster-Whisper –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç (CTranslate2)")
                            print(f"  ‚ö† –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Faster-Whisper –≤: {model_path}")
                            print(f"  ‚Ñπ –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –ø–æ—Å–ª–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –∏ –±–æ–ª—å—à–µ –Ω–µ –±—É–¥–µ—Ç —Å–∫–∞—á–∏–≤–∞—Ç—å—Å—è")
                        else:
                            print(f"  –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω–∞ –≤: {download_path}")
                    print(f"  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Faster-Whisper (—Ñ–æ—Ä–º–∞—Ç CTranslate2)")
                
                print(f"[LOAD_MODEL] –°–æ–∑–¥–∞–Ω–∏–µ WhisperModel –¥–ª—è {model_name}...")
                print(f"[LOAD_MODEL] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: device={self.device}, compute_type={'float16' if self.device == 'cuda' else 'int8'}, download_root={download_path}")
                try:
                    self.models[model_name] = WhisperModel(
                        model_name,
                        device=self.device,
                        compute_type="float16" if self.device == "cuda" else "int8",
                        download_root=download_path
                    )
                    print(f"[LOAD_MODEL] ‚úì WhisperModel —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                except Exception as e:
                    print(f"[LOAD_MODEL] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ WhisperModel: {e}")
                    import traceback
                    traceback.print_exc()
                    raise
            else:
                # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper
                print(f"  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper (—Ñ–æ—Ä–º–∞—Ç .pt)")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ WHISPER_CACHE_DIR
                whisper_cache = os.environ.get("WHISPER_CACHE_DIR")
                if whisper_cache:
                    print(f"  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫—ç—à: {whisper_cache}")
                    model_file = Path(whisper_cache) / f"{model_name}.pt"
                    if model_file.exists():
                        size_gb = model_file.stat().st_size / (1024 * 1024 * 1024)
                        print(f"  ‚úì –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ: {model_file} ({size_gb:.2f} GB)")
                    else:
                        print(f"  ‚ö† –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {whisper_cache}, –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω–∞")
                else:
                    print(f"  ‚ö† WHISPER_CACHE_DIR –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫—ç—à")
                print(f"[LOAD_MODEL] –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ Whisper: {model_name}")
                try:
                    self.models[model_name] = whisper.load_model(model_name)
                    print(f"[LOAD_MODEL] ‚úì –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                except Exception as e:
                    print(f"[LOAD_MODEL] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                    import traceback
                    traceback.print_exc()
                    raise
            
            print(f"[LOAD_MODEL] ‚úì –ú–æ–¥–µ–ª—å {model_name} –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
        return self.models[model_name]
    
    def transcribe(
        self,
        audio_path: str,
        language: Optional[str] = None,
        model: str = "base",
        beam_size: int = 5,
        best_of: int = 5,
        enable_diarization: bool = False,
        num_speakers: Optional[int] = None,
        speaker_names: Optional[List[str]] = None,
        translate_to_english: bool = False
    ) -> Dict:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Ä–æ–ª—è–º –∏ –ø–µ—Ä–µ–≤–æ–¥–æ–º –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ
            language: –∫–æ–¥ —è–∑—ã–∫–∞
            model: –º–æ–¥–µ–ª—å Whisper
            beam_size: —Ä–∞–∑–º–µ—Ä –ª—É—á–∞ (–º–µ–Ω—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ)
            best_of: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–º–µ–Ω—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ)
            enable_diarization: –≤–∫–ª—é—á–∏—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–æ–ª—è–º
            num_speakers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            translate_to_english: –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫
        
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # Diarization: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º WhisperX, –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: diarization —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (–Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å, –ø–æ—Ç–æ–º –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å)
        if enable_diarization and not translate_to_english:
            if WHISPERX_AVAILABLE:
                try:
                    return self._transcribe_with_diarization(
                        audio_path, language, model, num_speakers, speaker_names
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è  WhisperX diarization –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
                    print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é diarization –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—É–∑")
                    # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é diarization (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥)
                    if SIMPLE_DIARIZATION_AVAILABLE and not translate_to_english:
                        try:
                            return self._transcribe_with_simple_diarization(
                                audio_path, language, model, beam_size, best_of, speaker_names
                            )
                        except Exception as e2:
                            print(f"‚ùå –ü—Ä–æ—Å—Ç–∞—è diarization —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
                            print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ diarization")
                            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ–±—ã—á–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
                    else:
                        print("   –ü—Ä–æ—Å—Ç–∞—è diarization –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ —Ä–æ–ª—è–º")
            elif SIMPLE_DIARIZATION_AVAILABLE and not translate_to_english:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é diarization, –µ—Å–ª–∏ WhisperX –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥)
                try:
                    return self._transcribe_with_simple_diarization(
                        audio_path, language, model, beam_size, best_of, speaker_names
                    )
                except Exception as e:
                    print(f"‚ùå –ü—Ä–æ—Å—Ç–∞—è diarization –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ diarization")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ–±—ã—á–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (–±—ã—Å—Ç—Ä–µ–µ)
        print(f"[TRANSCRIBE] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model}...")
        whisper_model = self.load_model(model)
        print(f"[TRANSCRIBE] –ú–æ–¥–µ–ª—å {model} –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é...")
        
        # –í—Å–µ–≥–¥–∞ –¥–µ–ª–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ
        # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ - –¥–µ–ª–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤
        if translate_to_english:
            print(f"üåê –†–µ–∂–∏–º –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–∫–ª—é—á–µ–Ω: –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ –≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
        
        if FASTER_WHISPER_AVAILABLE:
            # Faster-Whisper API - —Å–Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ
            print(f"[TRANSCRIBE] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ (Faster-Whisper)...")
            print(f"[TRANSCRIBE] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: language={language}, beam_size={beam_size}, best_of={best_of}")
            try:
                segments, info = whisper_model.transcribe(
                    audio_path,
                    language=language,
                    beam_size=beam_size,
                    best_of=best_of,
                    vad_filter=True,  # Voice Activity Detection –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                    vad_parameters=dict(min_silence_duration_ms=500)
                )
                print(f"[TRANSCRIBE] ‚úì –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
            except Exception as e:
                print(f"[TRANSCRIBE] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
                import traceback
                traceback.print_exc()
                raise
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            segments_list = []
            full_text_parts = []
            
            for segment in segments:
                seg_dict = {
                    "id": len(segments_list),
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                }
                segments_list.append(seg_dict)
                full_text_parts.append(segment.text.strip())
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
            result = {
                "text": " ".join(full_text_parts),
                "language": info.language,
                "segments": segments_list,
                "has_translation": False
            }
            
            # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ - –¥–µ–ª–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper
            if translate_to_english:
                print(f"[TRANSLATE] –ù–∞—á–∞–ª–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π...")
                print(f"[TRANSLATE] ‚ö†Ô∏è  Faster-Whisper –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper")
                import whisper
                try:
                    standard_model = whisper.load_model(model)
                    print(f"[TRANSLATE] –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥...")
                    translate_result = standard_model.transcribe(
                        audio_path,
                        language=language,
                        task="translate",
                        beam_size=beam_size,
                        best_of=best_of
                    )
                    print(f"[TRANSLATE] ‚úì –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω")
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞
                    translated_segments = [
                        {
                            "id": seg.get("id", i),
                            "start": seg.get("start", 0),
                            "end": seg.get("end", 0),
                            "text": seg.get("text", "").strip()
                        }
                        for i, seg in enumerate(translate_result.get("segments", []))
                    ]
                    
                    result["translated_text"] = translate_result["text"].strip()
                    result["translated_language"] = "en"
                    result["translated_segments"] = translated_segments
                    result["has_translation"] = True
                except Exception as e:
                    print(f"[TRANSLATE] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ: {e}")
                    import traceback
                    traceback.print_exc()
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª
                    print(f"[TRANSLATE] ‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞")
                    result["has_translation"] = False
            
            return result
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper - —Å–Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ
            print(f"[TRANSCRIBE] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper)...")
            original_result = whisper_model.transcribe(
                audio_path,
                language=language,
                task="transcribe",
                beam_size=beam_size,
                best_of=best_of
            )
            
            original_segments = [
                {
                    "id": seg.get("id", i),
                    "start": seg.get("start", 0),
                    "end": seg.get("end", 0),
                    "text": seg.get("text", "").strip()
                }
                for i, seg in enumerate(original_result.get("segments", []))
            ]
            
            result = {
                "text": original_result["text"].strip(),
                "language": original_result.get("language", "unknown"),
                "segments": original_segments,
                "has_translation": False
            }
            
            # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ - –¥–µ–ª–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤
            if translate_to_english:
                print(f"[TRANSLATE] –ù–∞—á–∞–ª–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper)...")
                try:
                    translate_result = whisper_model.transcribe(
                        audio_path,
                        language=language,
                        task="translate",
                        beam_size=beam_size,
                        best_of=best_of
                    )
                    print(f"[TRANSLATE] ‚úì –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω")
                    
                    translated_segments = [
                        {
                            "id": seg.get("id", i),
                            "start": seg.get("start", 0),
                            "end": seg.get("end", 0),
                            "text": seg.get("text", "").strip()
                        }
                        for i, seg in enumerate(translate_result.get("segments", []))
                    ]
                    
                    result["translated_text"] = translate_result["text"].strip()
                    result["translated_language"] = "en"
                    result["translated_segments"] = translated_segments
                    result["has_translation"] = True
                except Exception as e:
                    print(f"[TRANSLATE] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ: {e}")
                    import traceback
                    traceback.print_exc()
                    result["has_translation"] = False
            
            return result
    
    def _transcribe_with_diarization(
        self,
        audio_path: str,
        language: Optional[str],
        model: str,
        num_speakers: Optional[int],
        speaker_names: Optional[List[str]] = None
    ) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Ä–æ–ª—è–º (—Ç—Ä–µ–±—É–µ—Ç WhisperX)"""
        if not WHISPERX_AVAILABLE:
            raise ImportError("WhisperX –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install whisperx")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—É—Ç–∏ –¥–ª—è HuggingFace, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        hf_home = os.getenv("HF_HOME")
        if hf_home:
            os.environ["HF_HOME"] = hf_home
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è HF_HOME: {hf_home}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        device = "cuda" if self.use_gpu and self.device == "cuda" else "cpu"
        
        # WhisperX –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É
        if self.cache_dir:
            # WhisperX –∏—â–µ—Ç –º–æ–¥–µ–ª–∏ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –º–µ—Å—Ç–µ –∏–ª–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
            os.environ["WHISPER_CACHE_DIR"] = str(self.cache_dir)
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è WHISPER_CACHE_DIR –¥–ª—è WhisperX: {self.cache_dir}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Faster-Whisper –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ)
        # –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º pyannote.audio –¥–ª—è diarization
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper: {model} (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Faster-Whisper –Ω–∞–ø—Ä—è–º—É—é
        if FASTER_WHISPER_AVAILABLE:
            from faster_whisper import WhisperModel
            download_path = str(self.cache_dir) if self.cache_dir else None
            whisper_model = WhisperModel(
                model,
                device=device,
                compute_type="int8" if device == "cpu" else "float16",
                download_root=download_path
            )
            print(f"–ú–æ–¥–µ–ª—å Whisper {model} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ Faster-Whisper")
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è...")
            segments, info = whisper_model.transcribe(
                audio_path,
                language=language,
                beam_size=5,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            segments_list = []
            for segment in segments:
                segments_list.append({
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                })
            
            result = {
                "language": info.language,
                "segments": segments_list
            }
        else:
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper
            import whisper
            whisper_model = whisper.load_model(model)
            result = whisper_model.transcribe(audio_path, language=language)
            result = {
                "language": result.get("language", "unknown"),
                "segments": [
                    {
                        "start": seg.get("start", 0),
                        "end": seg.get("end", 0),
                        "text": seg.get("text", "").strip()
                    }
                    for seg in result.get("segments", [])
                ]
            }
        
        print(f"‚úì –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(result['segments'])} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        
        # Diarization (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–æ–ª—è–º)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º HF_HOME –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫ E
        hf_home = os.getenv("HF_HOME")
        if hf_home:
            os.environ["HF_HOME"] = hf_home
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω HuggingFace (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
        hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º HuggingFace
        hf_home = os.getenv("HF_HOME", "/app/huggingface-cache")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ diarization —á–µ—Ä–µ–∑ pyannote
        # –í–ê–ñ–ù–û: –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–∫–µ–Ω Hugging Face –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —É—Å–ª–æ–≤–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        # –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens
        # –ü—Ä–∏–Ω—è—Ç—å —É—Å–ª–æ–≤–∏—è: https://hf.co/pyannote/speaker-diarization-3.1
        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å WhisperX.DiarizationPipeline, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if hasattr(whisperx, 'DiarizationPipeline'):
                print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è WhisperX.DiarizationPipeline...")
                if not hf_token:
                    print("‚ö†Ô∏è  –¢–æ–∫–µ–Ω Hugging Face –Ω–µ —É–∫–∞–∑–∞–Ω (HF_TOKEN –∏–ª–∏ HUGGINGFACE_TOKEN)")
                    print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è WhisperX diarization –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω.")
                    print("   –ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens")
                    print("   –ü—Ä–∏–º–∏—Ç–µ —É—Å–ª–æ–≤–∏—è: https://hf.co/pyannote/speaker-diarization-3.1")
                    print("   –î–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Spaces –∫–∞–∫ —Å–µ–∫—Ä–µ—Ç–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é HF_TOKEN")
                    raise ValueError("HF_TOKEN –Ω–µ —É–∫–∞–∑–∞–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏ diarization.")
                
                diarize_model = whisperx.DiarizationPipeline(
                    use_auth_token=hf_token,
                    device=device
                )
                
                if diarize_model is None:
                    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å diarization. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–æ–∫–µ–Ω –∏ —É—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.")
                    
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º pyannote.audio –Ω–∞–ø—Ä—è–º—É—é
                print("DiarizationPipeline –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ whisperx, –∏—Å–ø–æ–ª—å–∑—É–µ–º pyannote.audio...")
                print(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º: {hf_home}")
                
                if not hf_token:
                    print("‚ö†Ô∏è  –¢–æ–∫–µ–Ω Hugging Face –Ω–µ —É–∫–∞–∑–∞–Ω (HF_TOKEN –∏–ª–∏ HUGGINGFACE_TOKEN)")
                    print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è pyannote.audio diarization –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω.")
                    print("   –ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens")
                    print("   –ü—Ä–∏–º–∏—Ç–µ —É—Å–ª–æ–≤–∏—è: https://hf.co/pyannote/speaker-diarization-3.1")
                    raise ValueError("HF_TOKEN –Ω–µ —É–∫–∞–∑–∞–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏ diarization.")
                
                from pyannote.audio import Pipeline
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ Hugging Face
                diarize_model = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=hf_token,
                    cache_dir=hf_home
                )
                
                if diarize_model is None:
                    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å diarization. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–æ–∫–µ–Ω –∏ —É—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.")
                
                if device == "cuda":
                    import torch
                    diarize_model = diarize_model.to(torch.device("cuda"))
                
                print("‚úì –ú–æ–¥–µ–ª—å diarization –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as diarize_load_error:
            error_msg = str(diarize_load_error)
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ diarization: {error_msg}")
            if "HF_TOKEN" in error_msg or "token" in error_msg.lower() or "NoneType" in error_msg:
                print("\n" + "="*60)
                print("–†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
                print("="*60)
                print("1. –ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω Hugging Face:")
                print("   https://huggingface.co/settings/tokens")
                print("2. –ü—Ä–∏–º–∏—Ç–µ —É—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏:")
                print("   https://hf.co/pyannote/speaker-diarization-3.1")
                print("3. –î–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Spaces:")
                print("   Settings ‚Üí Secrets ‚Üí –î–æ–±–∞–≤—å—Ç–µ HF_TOKEN")
                print("="*60 + "\n")
            import traceback
            traceback.print_exc()
            raise
        
        print(f"‚úì –ú–æ–¥–µ–ª—å diarization –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ diarization
        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è pyannote.audio
        print(f"–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è diarization... (—Å–ø–∏–∫–µ—Ä–æ–≤: {num_speakers if num_speakers else '–∞–≤—Ç–æ'})")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–æ–π —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        # WhisperX.DiarizationPipeline –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥ min_speakers/max_speakers
        # pyannote.audio Pipeline - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ callable –æ–±—ä–µ–∫—Ç
        is_whisperx_pipeline = hasattr(diarize_model, 'min_speakers') or hasattr(diarize_model, '__class__') and 'DiarizationPipeline' in str(type(diarize_model))
        is_pyannote_pipeline = not is_whisperx_pipeline and hasattr(diarize_model, '__call__')
        
        if is_pyannote_pipeline:
            # –≠—Ç–æ pyannote.audio Pipeline - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π API
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è pyannote.audio Pipeline API...")
            from pyannote.core import Annotation
            
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è pyannote
                # pyannote.audio –æ–∂–∏–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "uri" –∏ "audio" (–ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –º–∞—Å—Å–∏–≤)
                diarize_input = {"uri": "audio", "audio": audio_path}
                if num_speakers:
                    diarize_input["num_speakers"] = num_speakers
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º diarization
                print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è diarization —á–µ—Ä–µ–∑ pyannote.audio...")
                diarization_result = diarize_model(diarize_input)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç pyannote –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤
                # pyannote –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Annotation –æ–±—ä–µ–∫—Ç
                diarize_segments_list = []
                
                if isinstance(diarization_result, Annotation):
                    # –≠—Ç–æ Annotation –æ–±—ä–µ–∫—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º itertracks –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                    # itertracks –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (segment, track, label)
                    try:
                        for segment, track, speaker in diarization_result.itertracks(yield_label=True):
                            diarize_segments_list.append({
                                "segment": {
                                    "start": float(segment.start),
                                    "end": float(segment.end)
                                },
                                "speaker": str(speaker)  # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ speaker - —Å—Ç—Ä–æ–∫–∞
                            })
                    except Exception as iter_error:
                        # –ï—Å–ª–∏ itertracks –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–±
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Ç–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ itertracks: {iter_error}")
                        print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±...")
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º get_timeline –∏ get_labels
                        timeline = diarization_result.get_timeline()
                        for segment in timeline:
                            labels = diarization_result.get_labels(segment)
                            speaker = str(list(labels)[0]) if labels else "SPEAKER_00"
                            diarize_segments_list.append({
                                "segment": {
                                    "start": float(segment.start),
                                    "end": float(segment.end)
                                },
                                "speaker": speaker
                            })
                elif isinstance(diarization_result, dict):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –º–æ–≥—É—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å dict)
                    for seg in diarization_result.get("segments", []):
                        diarize_segments_list.append({
                            "segment": {
                                "start": seg.get("start", 0),
                                "end": seg.get("end", 0)
                            },
                            "speaker": str(seg.get("speaker", "SPEAKER_00"))
                        })
                else:
                    # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫ –¥—Ä—É–≥–∏–º —Å–ø–æ—Å–æ–±–æ–º
                    print(f"‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ diarization: {type(diarization_result)}")
                    # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å get_timeline –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    if hasattr(diarization_result, 'get_timeline'):
                        timeline = diarization_result.get_timeline()
                        for segment in timeline:
                            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
                            labels = diarization_result.get_labels(segment)
                            speaker = list(labels)[0] if labels else "SPEAKER_00"
                            diarize_segments_list.append({
                                "segment": {
                                    "start": float(segment.start),
                                    "end": float(segment.end)
                                },
                                "speaker": str(speaker)
                            })
                    else:
                        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç diarization —Ç–∏–ø–∞ {type(diarization_result)}")
                
                print(f"‚úì Diarization –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {len(diarize_segments_list)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤")
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å diarization –≤—Ä—É—á–Ω—É—é
                print("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å diarization...")
                result = self._assign_speakers_manual(result, diarize_segments_list)
                print("‚úì –°–ø–∏–∫–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏—Å–≤–æ–µ–Ω—ã –∫ —Å–µ–≥–º–µ–Ω—Ç–∞–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
            except Exception as pyannote_error:
                error_msg = str(pyannote_error)
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ pyannote diarization: {error_msg}")
                import traceback
                traceback.print_exc()
                raise
        else:
            # –≠—Ç–æ WhisperX DiarizationPipeline - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π API
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è WhisperX DiarizationPipeline API...")
            try:
                # WhisperX DiarizationPipeline –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
                print(f"–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è diarization –¥–ª—è —Ñ–∞–π–ª–∞: {audio_path}")
                diarize_segments = diarize_model(
                    audio_path,
                    min_speakers=num_speakers if num_speakers else None,
                    max_speakers=num_speakers if num_speakers else None
                )
                
                print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç diarization: —Ç–∏–ø={type(diarize_segments)}")
                if hasattr(diarize_segments, '__len__'):
                    print(f"  –î–ª–∏–Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(diarize_segments)}")
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                diarize_segments_list = []
                from pyannote.core import Annotation
                import pandas as pd
                
                if isinstance(diarize_segments, pd.DataFrame):
                    # WhisperX DiarizationPipeline –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç pandas DataFrame
                    print("–†–µ–∑—É–ª—å—Ç–∞—Ç - pandas DataFrame")
                    print(f"  –ö–æ–ª–æ–Ω–∫–∏: {list(diarize_segments.columns)}")
                    print(f"  –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:\n{diarize_segments.head()}")
                    
                    # DataFrame –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–∫–∏: start, end, speaker
                    for _, row in diarize_segments.iterrows():
                        diarize_segments_list.append({
                            "segment": {
                                "start": float(row.get("start", row.get("start_time", 0))),
                                "end": float(row.get("end", row.get("end_time", 0)))
                            },
                            "speaker": str(row.get("speaker", row.get("label", "SPEAKER_00")))
                        })
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(diarize_segments_list)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ DataFrame")
                elif isinstance(diarize_segments, dict):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "segments"
                    print("–†–µ–∑—É–ª—å—Ç–∞—Ç - —Å–ª–æ–≤–∞—Ä—å")
                    segments = diarize_segments.get("segments", [])
                    print(f"  –ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(segments)}")
                    for seg in segments:
                        diarize_segments_list.append({
                            "segment": {
                                "start": seg.get("start", 0),
                                "end": seg.get("end", 0)
                            },
                            "speaker": str(seg.get("speaker", "SPEAKER_00"))
                        })
                elif isinstance(diarize_segments, Annotation):
                    # –ï—Å–ª–∏ —ç—Ç–æ Annotation –æ–±—ä–µ–∫—Ç
                    print("–†–µ–∑—É–ª—å—Ç–∞—Ç - Annotation –æ–±—ä–µ–∫—Ç")
                    try:
                        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å itertracks
                        for segment, track, speaker in diarize_segments.itertracks(yield_label=True):
                            diarize_segments_list.append({
                                "segment": {
                                    "start": float(segment.start),
                                    "end": float(segment.end)
                                },
                                "speaker": str(speaker)
                            })
                    except Exception as iter_error:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Ç–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ itertracks: {iter_error}")
                        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º get_timeline
                        timeline = diarize_segments.get_timeline()
                        print(f"  Timeline —Å–æ–¥–µ—Ä–∂–∏—Ç {len(timeline)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
                        for segment in timeline:
                            labels = diarize_segments.get_labels(segment)
                            speaker = str(list(labels)[0]) if labels else "SPEAKER_00"
                            diarize_segments_list.append({
                                "segment": {
                                    "start": float(segment.start),
                                    "end": float(segment.end)
                                },
                                "speaker": speaker
                            })
                else:
                    # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫
                    print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(diarize_segments)}")
                    # –ï—Å–ª–∏ —ç—Ç–æ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç, –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å
                    try:
                        for item in diarize_segments:
                            if isinstance(item, dict):
                                diarize_segments_list.append({
                                    "segment": {
                                        "start": item.get("start", 0),
                                        "end": item.get("end", 0)
                                    },
                                    "speaker": str(item.get("speaker", "SPEAKER_00"))
                                })
                    except Exception as iter_error:
                        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {iter_error}")
                        import traceback
                        traceback.print_exc()
                
                print(f"‚úì Diarization –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {len(diarize_segments_list)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤")
                
                if len(diarize_segments_list) == 0:
                    print("‚ö†Ô∏è  Diarization –Ω–µ –Ω–∞—à–ª–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤!")
                    print("   –í–æ–∑–º–æ–∂–Ω–æ, –∞—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞")
                    print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é diarization –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—É–∑ –∫–∞–∫ fallback")
                    # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é diarization
                    if SIMPLE_DIARIZATION_AVAILABLE:
                        return self._transcribe_with_simple_diarization(
                            audio_path, language, model, beam_size=5, best_of=5, speaker_names=speaker_names
                        )
                    else:
                        raise ValueError("Diarization –Ω–µ –Ω–∞—à–ª–∞ —Å–ø–∏–∫–µ—Ä–æ–≤ –∏ –ø—Ä–æ—Å—Ç–∞—è diarization –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å diarization –≤—Ä—É—á–Ω—É—é
                print("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å diarization...")
                result = self._assign_speakers_manual(result, diarize_segments_list)
                print("‚úì –°–ø–∏–∫–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏—Å–≤–æ–µ–Ω—ã –∫ —Å–µ–≥–º–µ–Ω—Ç–∞–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
            except Exception as diarize_error:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ diarization: {diarize_error}")
                import traceback
                traceback.print_exc()
                raise
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        segments = []
        speakers_text = {}  # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        
        for segment in result["segments"]:
            speaker = segment.get("speaker", "SPEAKER_00")
            seg_dict = {
                "id": len(segments),
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"].strip(),
                "speaker": speaker
            }
            segments.append(seg_dict)
            
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
            if speaker not in speakers_text:
                speakers_text[speaker] = []
            speakers_text[speaker].append(segment["text"].strip())
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        speakers_output = {
            speaker: " ".join(texts)
            for speaker, texts in speakers_text.items()
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        formatted_text_parts = []
        current_speaker = None
        current_text_parts = []
        
        for seg in segments:
            speaker = seg.get("speaker", "SPEAKER_00")
            text = seg.get("text", "").strip()
            
            if not text:
                continue
            
            if speaker != current_speaker:
                # –ù–æ–≤—ã–π —Å–ø–∏–∫–µ—Ä - –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–µ–∫—Å—Ç
                if current_speaker and current_text_parts:
                    speaker_name = self._format_speaker_name(current_speaker, speaker_names)
                    formatted_text_parts.append(f"{speaker_name}: {' '.join(current_text_parts)}")
                    current_text_parts = []
                current_speaker = speaker
            
            current_text_parts.append(text)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        if current_speaker and current_text_parts:
            speaker_name = self._format_speaker_name(current_speaker, speaker_names)
            formatted_text_parts.append(f"{speaker_name}: {' '.join(current_text_parts)}")
        
        formatted_text = "\n\n".join(formatted_text_parts)
        
        return {
            "text": " ".join([seg["text"] for seg in segments]),  # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
            "formatted_text": formatted_text,  # –ö—Ä–∞—Å–∏–≤—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
            "language": result.get("language", "unknown"),
            "segments": segments,
            "speakers": speakers_output,  # –¢–µ–∫—Å—Ç –ø–æ –∫–∞–∂–¥–æ–º—É —Å–ø–∏–∫–µ—Ä—É
            "num_speakers": len(speakers_output)
        }
    
    def _assign_speakers_manual(self, whisper_result: Dict, diarization_segments: List) -> Dict:
        """–í—Ä—É—á–Ω—É—é –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫"""
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å–ø–∏–∫–µ—Ä–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        speaker_map = {}
        for seg in diarization_segments:
            segment = seg.get("segment", {})
            speaker = seg.get("speaker", "SPEAKER_00")
            start = segment.get("start", 0)
            end = segment.get("end", start)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Ä–µ–¥–∏–Ω—É —Å–µ–≥–º–µ–Ω—Ç–∞ –∫–∞–∫ –∫–ª—é—á
            mid_time = (start + end) / 2
            speaker_map[mid_time] = speaker
        
        # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ –∫ –∫–∞–∂–¥–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        for segment in whisper_result.get("segments", []):
            seg_start = segment.get("start", 0)
            seg_end = segment.get("end", seg_start)
            seg_mid = (seg_start + seg_end) / 2
            
            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            closest_speaker = "SPEAKER_00"
            min_distance = float('inf')
            
            for time_key, speaker in speaker_map.items():
                distance = abs(time_key - seg_mid)
                if distance < min_distance:
                    min_distance = distance
                    closest_speaker = speaker
            
            # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–∞ –∫ —Å–µ–≥–º–µ–Ω—Ç—É
            segment["speaker"] = closest_speaker
        
        return whisper_result
    
    def _transcribe_with_simple_diarization(
        self,
        audio_path: str,
        language: Optional[str],
        model: str,
        beam_size: int,
        best_of: int,
        speaker_names: Optional[List[str]] = None
    ) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –ø—Ä–æ—Å—Ç—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Ä–æ–ª—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—É–∑ (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)"""
        if not SIMPLE_DIARIZATION_AVAILABLE:
            raise ImportError("–ü—Ä–æ—Å—Ç–∞—è diarization –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        whisper_model = self.load_model(model)
        
        if FASTER_WHISPER_AVAILABLE:
            # Faster-Whisper API
            segments, info = whisper_model.transcribe(
                audio_path,
                language=language,
                beam_size=beam_size,
                best_of=best_of,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            segments_list = []
            for segment in segments:
                segments_list.append({
                    "id": len(segments_list),
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                })
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Whisper
            result = whisper_model.transcribe(
                audio_path,
                language=language,
                task="transcribe",
                beam_size=beam_size,
                best_of=best_of
            )
            
            segments_list = [
                {
                    "id": seg.get("id", i),
                    "start": seg.get("start", 0),
                    "end": seg.get("end", 0),
                    "text": seg.get("text", "").strip()
                }
                for i, seg in enumerate(result.get("segments", []))
            ]
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç info —Å —è–∑—ã–∫–æ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Faster-Whisper API
            class Info:
                def __init__(self, lang):
                    self.language = lang
            info = Info(result.get("language", "unknown"))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        if not segments_list:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –∞—É–¥–∏–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∞—É–¥–∏–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ—á—å.")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç—É—é diarization —Å –æ—á–µ–Ω—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ 0.3 —Å–µ–∫ + –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
        segments_with_speakers = simple_diarization(segments_list, pause_threshold=0.3)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        unique_speakers = set(seg.get("speaker", "SPEAKER_00") for seg in segments_with_speakers)
        print(f"‚úì –ü—Ä–æ—Å—Ç–∞—è diarization –ø—Ä–∏–º–µ–Ω–µ–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {len(unique_speakers)} —Å–ø–∏–∫–µ—Ä–æ–≤")
        print(f"  –°–ø–∏–∫–µ—Ä—ã: {sorted(unique_speakers)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        speakers_output = group_by_speakers(segments_with_speakers)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ –º–µ—Ç–æ–∫ —Å–ø–∏–∫–µ—Ä–æ–≤)
        full_text = " ".join([seg.get("text", "") for seg in segments_with_speakers if seg.get("text")])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        formatted_text_parts = []
        current_speaker = None
        current_text_parts = []
        
        for seg in segments_with_speakers:
            speaker = seg.get("speaker", "SPEAKER_00")
            text = seg.get("text", "").strip()
            
            if not text:
                continue
            
            if speaker != current_speaker:
                # –ù–æ–≤—ã–π —Å–ø–∏–∫–µ—Ä - –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–µ–∫—Å—Ç
                if current_speaker and current_text_parts:
                    speaker_name = self._format_speaker_name(current_speaker, speaker_names)
                    formatted_text_parts.append(f"{speaker_name}: {' '.join(current_text_parts)}")
                    current_text_parts = []
                current_speaker = speaker
            
            current_text_parts.append(text)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        if current_speaker and current_text_parts:
            speaker_name = self._format_speaker_name(current_speaker, speaker_names)
            formatted_text_parts.append(f"{speaker_name}: {' '.join(current_text_parts)}")
        
        formatted_text = "\n\n".join(formatted_text_parts)
        
        return {
            "text": full_text,  # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –±–µ–∑ –º–µ—Ç–æ–∫
            "formatted_text": formatted_text,  # –ö—Ä–∞—Å–∏–≤—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
            "language": info.language,
            "segments": segments_with_speakers,
            "speakers": speakers_output,
            "num_speakers": len(speakers_output)
        }
    
    def generate_subtitles(self, transcription_result: Dict, format: str = "srt", include_speakers: bool = False) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—É–±—Ç–∏—Ç—Ä—ã —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤"""
        segments = transcription_result.get("segments", [])
        
        if format.lower() == "srt":
            return self._generate_srt(segments, include_speakers)
        elif format.lower() == "vtt":
            return self._generate_vtt(segments, include_speakers)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {format}")
    
    def _generate_srt(self, segments: List[Dict], include_speakers: bool = False) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç SRT —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ —Å–ø–∏–∫–µ—Ä–æ–≤"""
        srt_lines = []
        for seg in segments:
            idx = seg.get("id", 0) + 1
            start = self._format_timestamp(seg.get("start", 0))
            end = self._format_timestamp(seg.get("end", 0))
            text = seg.get("text", "")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É —Å–ø–∏–∫–µ—Ä–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
            if include_speakers and "speaker" in seg:
                text = f"[{seg['speaker']}] {text}"
            
            srt_lines.append(f"{idx}\n{start} --> {end}\n{text}\n")
        
        return "\n".join(srt_lines)
    
    def _format_speaker_name(self, speaker: str, speaker_names: Optional[List[str]] = None) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–º—è —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –∏–º–µ–Ω–∞ —Å–ø–∏–∫–µ—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if speaker_names and len(speaker_names) > 0:
            if speaker.startswith("SPEAKER_"):
                speaker_num = speaker.replace("SPEAKER_", "")
                try:
                    num = int(speaker_num)
                    if 0 <= num < len(speaker_names):
                        return speaker_names[num]
                except:
                    pass
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º SPEAKER_00 –≤ –±–æ–ª–µ–µ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        if speaker.startswith("SPEAKER_"):
            speaker_num = speaker.replace("SPEAKER_", "")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –Ω—É–º–µ—Ä–∞—Ü–∏—é (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é ML –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º–µ–Ω)
            try:
                num = int(speaker_num)
                if num == 0:
                    return "–°–ø–∏–∫–µ—Ä 1"
                elif num == 1:
                    return "–°–ø–∏–∫–µ—Ä 2"
                else:
                    return f"–°–ø–∏–∫–µ—Ä {num + 1}"
            except:
                return f"–°–ø–∏–∫–µ—Ä {speaker_num}"
        return speaker
    
    def _generate_vtt(self, segments: List[Dict], include_speakers: bool = False) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç VTT —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ —Å–ø–∏–∫–µ—Ä–æ–≤"""
        vtt_lines = ["WEBVTT\n"]
        for seg in segments:
            start = self._format_timestamp_vtt(seg.get("start", 0))
            end = self._format_timestamp_vtt(seg.get("end", 0))
            text = seg.get("text", "")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É —Å–ø–∏–∫–µ—Ä–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
            if include_speakers and "speaker" in seg:
                text = f"<v {seg['speaker']}>{text}</v>"
            
            vtt_lines.append(f"{start} --> {end}\n{text}\n")
        
        return "\n".join(vtt_lines)
    
    def _format_timestamp(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –¥–ª—è SRT"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
    
    def _format_timestamp_vtt(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –¥–ª—è VTT"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}.{millis:03d}"

