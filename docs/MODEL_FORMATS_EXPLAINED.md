# Объяснение форматов моделей Whisper

## Быстрый ответ

**Если вы видите загрузку `model.bin` (145MB+):**
- Это модель для **разделения по ролям** (diarization)
- Хранится в `HF_HOME` (например, `E:\huggingface-cache`)
- Это **нормально** при первом использовании функции "Разделение по ролям"
- Это **не связано** с вашими Whisper моделями (`medium.pt` и т.д.)

**Если Whisper модель (например, medium) загружается повторно:**

⚠️ **Скорее всего, у вас есть `medium.pt`, но используется Faster-Whisper!**

- У вас есть: `medium.pt` (формат стандартного Whisper)
- Используется: Faster-Whisper (формат CTranslate2)
- **Проблема:** Это разные форматы, несовместимые друг с другом
- **Решение:** Дайте Faster-Whisper загрузить модель один раз - после этого она сохранится
- См. раздел "Решения" ниже

---

## Проблема: "Модель уже скачана, но загружается снова"

Если у вас есть модель `medium.pt` в директории `E:\whisper-models\`, но она все равно загружается заново, это связано с **разными форматами моделей**.

## Типы моделей Whisper

### 1. Стандартный Whisper (openai-whisper)
- **Формат:** `.pt` файлы (PyTorch)
- **Пример:** `medium.pt`, `base.pt`, `large-v2.pt`
- **Где хранится:** В директории, указанной в `WHISPER_CACHE_DIR`
- **Используется в:** `SpeechRecognitionService` (если `faster-whisper` не установлен)

### 2. Faster-Whisper (оптимизированная версия)
- **Формат:** CTranslate2 (специальный бинарный формат)
- **Где хранится:** В подпапках типа `medium/` внутри `WHISPER_CACHE_DIR`
- **Используется в:** `OptimizedSpeechRecognitionService` (если `faster-whisper` установлен)
- **Важно:** Не может использовать `.pt` файлы напрямую! Модели хранятся в другом формате.

### 3. Модели Diarization (разделение по ролям)
- **Формат:** HuggingFace моделей (`.bin` файлы)
- **Пример:** `model.bin` (145MB+)
- **Где хранится:** В `HF_HOME` (например, `E:\huggingface-cache`)
- **Используется для:** Определения говорящих (кто говорит)

## Почему модель загружается повторно?

### Сценарий 1: Faster-Whisper не находит модель в своем формате

Если вы видите в коде:
```
✓ Используется оптимизированный сервис распознавания
```

Это значит используется **Faster-Whisper**. Даже если у вас есть `medium.pt`, Faster-Whisper его не использует, так как:
- Он ищет модель в формате CTranslate2
- Он ищет в папке типа `E:\whisper-models\medium\` (не сам `.pt` файл)
- Он не может конвертировать `.pt` файл автоматически

**Решение:**
- Вариант A: Дать Faster-Whisper загрузить модель в своем формате (первый раз займет время)
- Вариант B: Использовать стандартный Whisper (удалить `faster-whisper` или переименовать)

### Сценарий 2: Загружается модель Diarization

Если вы видите:
```
model.bin:   0%|          | 0.00/145M [00:00<?, ?B/s]
Используется HF_HOME: E:\huggingface-cache
```

Это загружается модель для **разделения по ролям** (diarization). Это нормально:
- Модель хранится в `HF_HOME`, а не в `WHISPER_CACHE_DIR`
- Это другая модель, не связанная с `medium.pt`
- Загружается только при использовании функции "Разделение по ролям"

## Как проверить, какая модель загружается?

### 1. Проверьте, какой сервис используется

В консоли при запуске бэкенда смотрите:
```
✓ Используется оптимизированный сервис распознавания
Используется: Faster-Whisper
```

или

```
⚠ Используется стандартный сервис. Для ускорения установите: pip install faster-whisper
```

### 2. Проверьте структуру директорий

**Для стандартного Whisper:**
```
E:\whisper-models\
  ├── medium.pt          ← Модель здесь
  ├── base.pt
  └── ...
```

**Для Faster-Whisper:**
```
E:\whisper-models\
  ├── medium\            ← Модель здесь (в папке)
  │   ├── config.json
  │   ├── model.bin
  │   └── ...
  ├── base\
  └── ...
```

**Для Diarization (HuggingFace):**
```
E:\huggingface-cache\
  └── hub\
      └── models--pyannote--speaker-diarization-3.1\
          └── ...
              └── model.bin    ← Модель здесь
```

## Решения

### Вариант 1: Использовать существующие .pt модели (стандартный Whisper)

Если у вас уже есть `.pt` файлы и вы хотите их использовать:

```powershell
# Удалить faster-whisper (чтобы использовать стандартный Whisper)
cd backend
.\venv\Scripts\Activate.ps1
pip uninstall faster-whisper -y

# Перезапустить сервер
.\run_server.ps1
```

Теперь будет использоваться `SpeechRecognitionService`, который умеет работать с `.pt` файлами.

**Плюсы:**
- Использует существующие модели
- Не нужно ничего скачивать заново

**Минусы:**
- Медленнее, чем Faster-Whisper
- Больше потребление памяти

### Вариант 2: Дать Faster-Whisper загрузить модели в своем формате

Оставить как есть и позволить Faster-Whisper загрузить модели в формате CTranslate2. Первый раз займет время, но потом будет работать быстрее.

**Плюсы:**
- Быстрее работает
- Меньше потребление памяти

**Минусы:**
- Нужно загружать модели заново (но только один раз)
- Занимает больше места (модели в двух форматах)

### Вариант 3: Конвертировать существующие .pt модели в CTranslate2

Можно конвертировать `.pt` модели в формат Faster-Whisper:

```powershell
cd backend
.\venv\Scripts\Activate.ps1

# Установить ct2-transformers-converter
pip install ctranslate2

# Конвертировать модель (пример)
# Примечание: это требует дополнительных шагов, см. документацию ctranslate2
```

Это более сложный вариант, обычно не требуется.

## Проверка текущего состояния

Запустите скрипт проверки:

```powershell
cd backend
.\venv\Scripts\Activate.ps1
python check_model_paths.py
```

Он покажет:
- Какие пути настроены
- Какие модели найдены
- Где ищет Faster-Whisper

## Рекомендация

**Если модель `model.bin` загружается из HuggingFace:**
- Это нормально! Это модель для diarization
- Она сохранится в `E:\huggingface-cache` и больше не будет скачиваться
- Это не связано с `medium.pt`

**Если Whisper модель загружается повторно:**
- Убедитесь, что `WHISPER_CACHE_DIR` правильно установлен
- Проверьте, какой сервис используется (стандартный или Faster-Whisper)
- Если используете Faster-Whisper, дайте ему загрузить модель в своем формате (один раз)

